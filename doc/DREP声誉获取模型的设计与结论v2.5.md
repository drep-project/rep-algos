# DREP声誉获取模型的设计v2.5

tag：声誉  数值模拟

本文通过对不同类型的用户模型进行声誉模拟，并进行统计学分析，得到相应类型用户预期的累计声誉获得图与获得相应累计声誉预期时间图，为合作平台使用声誉系统，设计相应奖励、荣誉等机制提供了相应的技术支持。同时模型针对链上特性进行了调整与优化，可以期待在DREP子链体系中得到一定的应用。

## 总述

商家为了激励用户活动与消费，往往会采用积分系统来激励客户进行多次交易。积分系统在激励与回馈的同时，还能引导用户养成活动习惯，逐步使用户产生依赖性，提升用户黏度与留存。根据路透社2018年的分析[1]表明，2017年全球数字积分市场的估值达到19.9亿美元，预测增长率达到17.5%，到2023年将达到52.4亿美元。由于获取新用户比起维护老用户的成本之比为5倍以上，同时老用户再次消费的比例也比新用户高得多[2]，积分作为低成本维护老用户的方法在市场中得到重视。

在数字积分市场高速发展的同时，也出现了传统积分体系与现今用户需求的矛盾——透明度差[3]、可用性差[4]等痛点。鉴于此，DREP将通过区块链、去中心化数字ID、声誉协议等因素，为商家和用户提供数据高度安全、过程公开透明，方便使用的reputation生态系统。这个系统将为众多合作平台提供具有真正使用价值的声誉，以此确定可以交易token的价值。与此同时，高声誉用户在相应平台能获得较大的影响力/优惠等优势，鼓励用户提高声誉。

区块链+声誉系统对传统的积分系统是一个较大的改进与提升，体现在：

- 声誉系统能够真正反映用户行为的有效性。目前互联网积分系统中，对于除了登录等简单机制之外的用户行为，往往很难真正得知用户的行为质量，同时还存在刷分等不利于积分体系正常运行的行为[5]。在声誉系统下，用户进行评论等复杂声誉活动的时候，需要经过他人的声誉评价等更加公平有效的机制方能获得声誉，能够真正反映用户的评论质量。同时举报等机制也必须在抵押自身声誉的情况下进行，既避免了恶意举报导致的人人自危的现象，也保障用户举报的动力。
- 区块链能够有效减少作弊可能。用户任何获取声誉的行为，均在区块链上有所记录。区块链具有不可篡改的特性[6]和时间戳这一利器，这样如果用户发生作弊的行径，在区块链上很容易查到相应的记录。同时，可以将刷分的行为更好的发现，调整相应的声誉系统以减少低层次的获得声誉。
- 声誉系统能够赋予积分以价值。由于声誉获得不容篡改，这样获得的声誉具有真正的价值。当投射到积分上时，可以通过DREP侧链发行相应的token，token可以进行用户在同平台甚至跨平台跨链交易，从而使用户真正将积分使用起来。商家之间可以调整token的汇率和兑换奖励，整合各个分散的积分，使用户能够提高粘附性的同时换取真正想要的奖励。

由于声誉挖矿、平台声誉体系等种种关于reputation的系统的建立初衷仍然是增强用户对于平台的忠诚度，达到更好的用户管理。所以，用户活跃RFE模型[7]仍然具有参考价值。RFE模型通过对用户上次使用时间（Recent time）、使用频率（Frequency）和互动程度（Efficient）进行分层，将不同类型的用户进行分析处理，从而得出针对性提高用户留存和吸引新用户的方案。RFE可以较为动态地显示了一个客户的全部轮廓，对进一步服务提供了依据。同时，随着时间发展，可以逐渐精确地判断该客户的长期价值(甚至是终身价值)，通过改善三项指标的状况，从而为更多的决策提供支持。

但这个模型具有以下不足之处：

- 量化性差。我们使用的声誉模型除了对用户管理的层面以外，还具有在声誉挖矿等奖池分配方面量化收益的重要参数，在平台将要进行的各种活动中声誉也会有量化的必要性和重要性（比如需要声誉超过若干方可参加活动等）。在这个方面，RFE模型不具备相应的量化处理方法，只是简单的进行分区或者求和作为分析手段。这不足以满足声誉系统的需求。
- 时间尺度太大。由于传统用户行为统计往往以天计算，而区块链加入系统之后，每次用户进行声誉相关操作之后，会在下一个区块上加以记录，这个时间从数秒到数分钟不等，不论如何均远远小于一天的尺度。为了让用户能够及时得知声誉变化情况，商家分析相应时间段用户活动情况，需要一个更加即时的声誉累计系统。
- 与积分系统的联系不大。Efficient和积分联系较为单一，而整体统计和积分的相关性往往也很难调节。

为此，DREP开发团队在进行相应声誉系统开发的同时，对相应系统进行数值模拟，对获得声誉和累计声誉等维度量化处理，从而获得了既能反映短期声誉获取不同又具有长期分析特征的结果。通过对结果的分析，可以得出真实用户所对应的模型参数，对之后的决策给予建议与支持。

## 模型说明

模型分为两个版本，版本1为开发者使用，允许输入更复杂的参数或者输入真实数据来研究相应用户声誉累计。版本2为用户使用，能够更加方便直接得到相应类型用户的声誉累计与相应声誉所需累计时长等关键信息。

### 理论模型

基于以下分段EMA模型计算累计声誉：

$$
Rep_{total}(t)=\sum_0^n\sum_{start\tau}^{end\tau}\alpha_i^{\tau}Rep_{gain}(t-\tau)\times(1+\beta Time_{active}^{\gamma})
$$
其中各符号含义如下：

Rep_gain (t-τ)表示时间点t-τ获得的声誉，Rep_total则是累计声誉，0到n为n+1个回归系数分段，α是通过各分段下衰减的最终值计算出的回归系数，β和γ则是与活跃总时长相关的系数，Time_active为累计总活跃时长。

在RFE模型中各个因子均在模型计算中得到了体现：

- R因子——上次活跃时间：距离当前越远作用越小，距离当前越近作用越大。且因为模型为分段EMA，短期内一次活跃/不活跃对结果影响较大，但经过时间推移将会逐渐消失。
- F因子——频率通过对不同尺度的回归中也有体现，同时频率与活跃总时长成正比。
- E因子——活跃程度正是以声誉获得来体现，用户通过不同的行为来获得声誉，可以根据平台上相关设计将累计积分转化为声誉积分。相比于简单的RFE分类方法，定量化的EMA声誉模型能够更加精确地分析用户的行为价值。

公式的前半段通过分段指数移动平均（EMA）对之前声誉获得进行累计，目前设计为前1/3接近线性移动，后面则完全是指数移动，用来衡量对于活跃程度不太高的用户或离开一段时间用户的残余声誉。后半段则是对活跃程度较高的用户的激励措施，通过用户的累计活跃时长幂函数上升累计声誉，从而保持长期声誉上升稳定性，鼓励用户减少断签，增加活跃度。

为了验证模型的有效性，均使用随机模型进行验证，开发者版本也可以用test_file.txt导入各种数据。

#### Token奖励模型

考虑到累计声誉需要与相应子链的token奖励结合，考虑到响应性、奖励的一致性等诸多因素，采用以下公式计算第t个时间点获得的奖励Token数量：
$$
Token(t)=k\times Rep_{total}(t)*Rep_{gain}^2(t)/(Rep_{gain}(t-1)+1)/100/(Time_{active}+1)
$$
其中k是根据不同实际需要调节的参数。

### 模型性质与证明

#### 长期增性

回归模型在各种时间序列分析中均得到应用，但在声誉模型计算中会发现这样一个问题：当用户的活跃程度稳定之后，长期来看获得的声誉是一个与活跃程度f，活跃时平均获得声誉E(Rep_active)相关的确定值：
$$
E(Rep_{cum})=f*E(Rep_{active})*\sum_0^n\sum_{start\tau}^{end\tau}\alpha_i^\tau
$$
而对累计声誉距离期望差小于ε所需的时间期望
$$
E(t||Rep_{cum}(t)-E(Rep_{cum})|<\epsilon))=\int_0^{\infin}tP(|Rep_{cum}(t)-E(Rep_{cum})|<\epsilon) dt
$$
鉴于模型的复杂性，我们选择使用数值方法进行计算，在1000天范围之内可以得出当f>0.32时相应期望t在1000天之内，随着f的上升所需时间也会减少。这就对高活跃频率的用户无法起到任何的激励作用。

于是我们采用累计活跃天数对高活跃玩家的累计声誉进行奖励：
$$
Augment\ Factor=1+\beta Time_{active}^{\gamma}
$$
当用户满足活跃相关要求时，累计活跃天数增加1，反之则对整体天数乘以一定系数ε作为惩罚，这样就给与低活跃用户的累计声誉增幅并不多，而对高活跃用户的累计声誉增幅比较明显。从而在整体上，保证用户在很长时间内均会发现自己声誉的长期增性，激励用户的长期活跃行为。

#### 相关性

累计声誉的获取与Token奖励的释放均应该与声誉的获得成正相关，但这种相关性应当是有限的，如果相关性过高，那么完全有理由用更少的变量替代，降低区块链运行成本；而相关系数过低，则导致用户难以将自身行为与显现的结果联系起来，影响激励动力，不利于积分生态建设。

根据计算可以得知：
$$
\eta(Rep_{gain},Rep_{cum})=\frac{Cov(Rep_{gain},Rep_{cum})}{\sqrt{Var(Rep_{gain})Var(Rep_{cum})}}=0.1344\\
\eta(Rep_{cum},Token_{sum})=\frac{Cov(Rep_{cum},Token_{sum})}{\sqrt{Var(Rep_{cum})Var(Token_{sum})}}=0.4786
$$
说明Token的获得和累计声誉的关系较大，而单个时间点的声誉获取与累计声誉的关系并不大。相对应的，如果将回归时间段的回归声誉与累计声誉的相关性进行计算可以得到：
$$
\eta(Rep_{sum1},Rep_{cum})=\frac{Cov(Rep_{sum1},Rep_{cum})}{\sqrt{Var(Rep_{sum1})Var(Rep_{cum})}}=0.8658
$$
具有很高的相关性。考虑到这个sum1是t0天内的回归声誉，说明累计声誉能够对最近较长天的声誉获得做出响应——只有连续的高活跃才能够激活累计声誉的上升，单独1天的高活跃/低活跃没有太多影响。

#### 历史可回溯性

由于去中心化的要求，不能因为单个节点的数据损毁导致整体的数据损失。与此同时，已经打包进区块的数据是通过Merkle Tree进行的，查询一个用户的长期交易数据需要进行全历史区块解包，非常消耗时间，成本也很高。模型的一大优势正是可以通过较少的已知量推算出用户从始至终得全部声誉变化，同时这些已知量具有较好的实际意义，对于用户也容易理解。

在实际应用中，我们选择两段回归计算，拐点日为t0天。从而需要存储在区块链上的状态有：

累计声誉（Cumulative Reputation as Reptotal）

最近t0天内的回归声誉（Regressive Reputation in recent t0 days as Rept0）,t0天外的回归声誉记为Reppast

累计活跃天数（Cumulative Active Days as Timeactive）

证明如下：

$$
Rep_{total}(t)/(1+\beta Time_{active}^{\gamma})=Rep_{t_0}(t)+Rep_{past}(t)
$$
从而可以用数学归纳法计算出各个时间的声誉获取：
$$
t=0,Rep_{t_0}(t)=Rep_{gain}(t)\\
t<t_0,Rep_{t_0}(t)=Rep_{gain}(t)+\alpha_1Rep_{t_0}(t-1),\\thus\ Rep_{gain}(t)=Rep_{t_0}(t)-\alpha_1Rep_{t_0}(t-1)
$$
然后当t>=t0时：
$$
Rep_{t_0}(t)=Rep_{gain}(t)+\alpha_1[Rep_{t_0}(t-1)-r_0Rep_{gain}(t-t_0+1)]\\
Rep_{past}(t)=\alpha_2[Rep_{past}(t-1)+r_0Rep_{gain}(t-t_0+1)]\\
thus\ Rep_{gain}(t)=Rep_{t_0}(t)-\alpha_1[Rep_{t_0}(t-1)-r_0Rep_{gain}(t-t_0+1)]
$$
t0天前的数据显然可以从t<t0的数据获得，从而可以恢复用户历史上全部的声誉数据。

### 声誉统计

对于用户而言，完全版本的声誉累计模型调节参数非常复杂，结果由于随机数的影响波动往往也较大，不便于分析相应累计声誉特征。为此，DREP提供了相应统计模块，通过多个样本的分析生成对于相应类型用户累计声誉的统计图，并在图中以误差棒标记一段时间累计声誉的标准差。

模型参数通过setting.ini进行输入，可以通过统计之前用户活跃期获得的活跃分数比例、偏差和活跃频率获得相应统计结果。

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_stat.png)

一个对活跃频率=50%，以1小时为单位进行50次样本统计的结果示意图

### 时间预测

作为商家而言，声誉往往会对应相应的声誉等级和声誉特权。为了让设计更加合理，方便用户调用，设计了相应的需要多少时间获得相应累计声誉的应用。

模型参数通过setting2.ini读入，用来确定需要多少时间才能获得相应的累计应用。如果显示最大时间且误差棒偏差为0，表明在相应时间段内不可能达到相应的声誉值。

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_time.png)

对目标1000累计声誉的统计，表明在用户活跃频率在40%之前，基本没有可能在1000天之内获得1000累计声誉。随着活跃频率的上升，所需天数相应减少。这样，就可以根据相应的临界值设定相应的声誉等级。

### 精确模拟

在开发模式下可以对用户上线若干天之后离开或者用户改变了自身的上线频率等不同情况进行模拟，根据数据变化商家可以对之后的累计声誉进行更加精细的预测和控制。

相应参数含义：

- Timeunit/s：表示每个时间间隔经过了多少秒，推荐60（1分钟）到86400（1天）。
- Day_max：模拟的天数。
- Reputation_max：单个时间间隔内最大允许获得多少Rep。
- Rep_lowerlimit_as_active：定义用户单个时间间隔活跃的标准，即达到若干Rep算活跃。
- Regression_endday：EMA截断日，表示超过相应日期就不考虑进移动平均。
- Attenuation_speed：EMA衰减速度，每个分段k，p表示经过k天之后权重下降到p。
- Bottom_factor：对启用声誉最终不归零而是恢复到一个确定数值的机制（简称为保底）中，最终声誉与最后活跃日声誉的比值。
- RandomForm：随机数据符合的分布，正态分布为‘n’，F-分布为‘f’。
- RandomPara：随机数据参数，n下分别为平均值与方差，f下为dfd和dfn。
- RandomProb：在相应模拟时长内可以决定各段的活跃频率。

一个示意图如下，事实上上面的统计正是在此结果之上进行：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_sim.png)

一个在之前1/3以50%频率活跃用户，中间1/3完全沉默，后面1/3以40%频率活跃用户的累计声誉计算模拟

## 结果分析

### 默认参数与灵敏度分析

默认参数随机结果如下（蓝线为随机声誉获得序列，红色为累计声誉）：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_std.png)

默认参数：以1天作为单位，活跃频率50%，回归衰减速度slow的示意图

对于不同因子对声誉获取模型的影响，选择灵敏度分析是较为合适的，控制单一因子变化，研究对结果的影响程度。考虑到实现方式和计算代价，选择Morris法[8]较为合适。

#### 不同时间尺度的图像

以1小时作为尺度的图像：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_hour.png)

以1分钟作为尺度的图像：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_minute.png)

在排除随机数造成的差别之后可以认为，在不同的时间尺度下基本达成一致的结果，均是在震荡中逐步上升。这足以说明声誉获取模型对时间尺度变化的灵敏性较低，换句话说系统具有跨不同时间尺度的通用性。

#### 不同衰减系数的不同

在衰减速度一项，具有5个档位：very fast、fast、medium、slow和very slow五种。不同的档位下累计声誉的数值和波动程度会具有一定的变化。下面以一个相同随机数据源的计算来讨论：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_veryfast.png)

very fast效果示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_fast.png)

fast效果示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_medium.png)

medium效果示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_slow.png)

slow效果示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_veryslow.png)

very slow示意图

可以看出，在随机声誉获取数据完全相同的情况下，不同衰减速度下图像具有一定的相似性，但有两点明显的不同：

- 从very fast到very slow，图像的上下波动越来越小，对短期波动从非常灵敏到不太灵敏。
- 累计声誉的值从约500逐渐上升到约1200。

通过多个档位，可以对不同情况的累计声誉获取进行调整，更加贴近实际情况。

附注：不同档位的衰减因子和截断时间（之后不再计算）：

| Attenuation speed | Truncation Day | Attenuation factor after 1/3 TD |
| ----------------- | -------------- | ------------------------------- |
| very fast         | 45             | 0.5                             |
| fast              | 60             | 0.5                             |
| medium            | 75             | 0.6                             |
| slow              | 90             | 0.7                             |
| very slow         | 120            | 0.8                             |

#### 不同活跃频率对累计声誉的影响

用户不同的活跃频率对累计声誉的影响非常大，如下所示：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.2.png)

f=0.2示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.3.png)

f=0.3示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.4.png)

f=0.4示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.5.png)

f=0.5示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.6.png)

f=0.6示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.7.png)

f=0.7示意图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.8.png)

f=0.8示意图

简单来说，增加0.1的频率（即一周活跃次数增加约0.7次），累计声誉大约提高200。在f<=0.3下，由于较难达到稳定点，增长较为明显但总值很低。f居中时表现出一定的增长，f较大（如f>=0.6时)下则会看到较为明显的增长，不过仍然在可控范围。这就鼓励用户提高活跃频率，加速自身的累计声誉获取。

#### 不同随机模型对结果的影响

默认模型是正态分布，正态分布的平均值与标准差会影响到结果，而F-分布也会影响到结果。

在F分布下结果如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_f.png)

F分布下由于有更大的概率出现边界值，所以F分布应该比正态分布更加分散与激烈。目前的算法可以很好地平衡短期的较大变动和长期的稳定。

将正态分布的平均值上升到40结果如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_40.png)

可以看出，改变平均值会提高累计声誉的获得，但变化趋势与之前相似，都是逐渐上升。

提高标准差到15的结果如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_15.png)

标准差的增大必然伴随着小概率事件的增加，这种波动在系统中得到了很好的体现。而平均值没有变化，应该从长期而言累计声誉也不会有较大的变化，这也是和之前相符的。

#### Morris法灵敏度分析

除了随机模型的分布特性无法转化为Morris法参数以外，其余变量均可以Morris法进行分析，得出相应的灵敏性系数：
$$
Matrix_{Morris}[Scale_{time}\ f\ \alpha_1\ \alpha_2\ \beta\ \gamma]= \overrightarrow{Result}
$$
得出计算结果如下：

| 灵敏度 | 结果      |
| ------ | --------- |
| Scale  | 8.64*10^2 |
| f      | 1.80*10^3 |
| α1     | 4.92*10^3 |
| α2     | 9.20*10^2 |
| β      | 1.51*10^3 |
| γ      | 1.55*10^4 |

说明主要的影响因素是累计活跃天数的指数，其次则是回归声誉的最靠近部分。

### EMA衰减系数与相应累计声誉归零的讨论

考虑到真实情况，我们对不同时期离开相应平台后累计声誉的变化的分析如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_3_end.png)

可以看出，在30天内仍然会保有一定程度的累计声誉，30天到60天以快速指数下滑的方式将累计声誉归零。

另一个例子则是30天后离开相应平台：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_30_end.png)

即使使30天都活跃的优秀用户，其声誉大约在第90天归零，同时其下降速度也更加和缓，符合一般认知。

再考虑后期用户回归的情况：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_30_100.png)

回归后速度只比原先稍快，即说明这个系统即能很好的反映之前有声誉累计，老用户回归有优势，同时这个优势并不大，不足以影响新用户的热情。同时，根据之前的活跃天数积累，前面活跃的30天只会残余5天，在损失了25天的活跃天数后已经和持续活跃用户拉开了差距。

这足以说明这个累计声誉系统能够很好的满足实际中能够出现的各种特殊情况。

### 累计声誉模拟统计的讨论

在实际应用中，过分关注单一用户的行为并没有太大的意义，重点应该在这一类用户的通用特征，根据不同用户的特征进行针对性策略调整。目前很多软件可以归纳出相应的因素，但无法对改变因素后未来的情况做出预测。对于平台而言，一个不成功的调整方案可能会造成经济损失和可信度损失。RFE虽然做的更早，但缺少定量化分析，同时它必须基于既有统计数据才能进行，无法对更多的现象进行预测。

之前的声誉累计模拟获得的结果往往由于样本太少而不够稳定，故对模拟进行多次采样，获得较为明显的统计样本进行深入分析。

#### 不同活跃频率下平均结果的分析：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_a_0.2.png)

f=0.2结果图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_a_0.4.png)

f=0.4结果图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_a_0.6.png)

f=0.6结果图

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_a_0.8.png)

f=0.8结果图

根据不同的结果，就可以知道在调整之后未来的变化情况。如果需要模拟更复杂的情况，开发者版本可以用于进一步样本的生成。

### 声誉所需时间预测的讨论

在设计声誉系统的时候，往往会遇到这样一个问题：如果需要用户到达若干声誉后获取优惠/特权，或者像声誉等级，那么究竟需要多久才能获取足够的声誉？这个问题对于累计声誉也十分重要。所以同样提供相应的声誉时间预测系统：

预测1000声誉所需时间的图像如下：

 ![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_time.png)

说明至少需要40%活跃（换言之，每周活跃3次左右）才能在1.5年左右获得1000累计声誉，最少也需要1年时间。但对于非常活跃的用户（每周活跃6次或更高），大约需要数月时间即可。这就明显造成了对用户的激励：

对一个每周活跃3次的普通用户，可以提醒提升活跃次数，可以缩减获得1000声誉三分之一的时间。商家也可以更好的预测不同类型的用户在一定的时间获得的声誉，从而更好的确定给与哪些优惠/特权。

获得2000声誉的时间示意图：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_time2000.png)

从这能看出，用户活跃频率需要达到每周4次以上才有可能获得2000声誉，随着频率上升所需时间呈现指数下降。这就表明累计2000声誉用户将会很稀少，这样的用户可以赋予较高的声誉等级，提高在相应平台的影响力。

获得500累计声誉的示意图：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/Rep_time500.png)

至少需要每周活跃1次以上才能在1年之后获得500累计声誉，之后随着频率上升呈现指数下降，是比较符合常理的。

### 与不使用相应算法的比较

为了说明算法本身的优势，在假设获得声誉的算法不变的情况下，比较不使用相应算法（即线性累加）和使用回归算法的优劣。

计算结果图如下(参数与默认设置保持一致）：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_dif2.png)

可以看出在f=0.5下，累加算法很难表现出某日是否活跃产生的不同，相反回归算法对于某日活跃的响应非常明显。同时其变化趋势为线性上升，和其他f的不同只能从累加声誉数值上的不同得以体现（下图为f=0.3情况）：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_dif3.png)

从之前不同活跃频率对累计声誉的影响图可以得知，f的轻微变化不仅会影响最终获得的累计声誉，还会明显地影响到声誉获得曲线的走势。相比而言，回归声誉的设计更加合理。

而如果出现长期不活动状态，线性累加的声誉表现合理性很低：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_dif1.png)

在长达70天的不活跃期，由于不会减少，所以累加声誉保持不变，并可以为后续活动获得声誉进行无损的初值积累。而回归声誉在70天后归零，后面再次活跃的声誉必须重新积累。从该图也可看出，早期累加声誉与回归累计声誉的差别并不大，但后期变化很大，方便根据不同的获得进行声誉调整，而不必担心由于数值积累过快而导致数值膨胀。

最后对比两个不同的活跃频率对两个算法的差别：

| 活跃频率 | 1000日累计声誉 | 1000日回归声誉 |
| -------- | -------------- | -------------- |
| 0.2      | 6600           | 180            |
| 0.4      | 12500          | 400            |
| 0.5      | 16000          | 520            |
| 0.6      | 20000          | 720            |
| 0.8      | 27000          | 1250           |

可以看出，实际上回归声誉拉大了不同活跃频率之间获得声誉的相对比值，使不同活跃频率的用户之间的区分度更大，更有利于用户获得高声誉产生兴奋感，也有利于商家对不同用户的策略区分。

### 保底值与回归减缓设计

鉴于某些需求中可能会对老用户提供一定的基础声誉值作为纪念，而不希望他们回归后发现需要重新开始，提供一个保底值与回归减缓的设计。

保底值：当最后一次活跃后一直不再获取Rep后，累计声誉降低的下限。到了保底值，累计声誉不再下降，但在重新变得活跃前丧失未来参与奖励等活动的权利。
$$
Rep_{remain}=Rep_{gain}(t_{lastactive})*bottom\ factor
$$
同时为了挽留老用户，其回归曲线的后面较为陡峭的一段会变得更加和缓：
$$
\alpha_n'=\alpha_n^{(t_n-t_{n-1})/(t_n-t_{n-1}+[k*Rep_{remain}^l])}
$$
结果图如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_1000_end_0.5.png)

在1000天后停止获取声誉，声誉在50天左右停留在1200的10%附近，同时这也比无此设置的结果更加缓慢：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_1000_end_0.5_no.png)

不过即使有此设置，其累计声誉损失速度仍然很快，不能当作纯粹给老用户的福利。

### 奖励模型的讨论

对默认模型进行Token奖励计算的结果如下：

![](https://github.com/drep-project/rep-algos/blob/master/imagesources/pic_0.5_token.png)

图中蓝线为累计声誉，红线为累计Token奖励。可以看出整体为凹函数，早期奖励较为明显，后期则为线性增长。而在活跃度较小，累计声誉下降时，Token获得为0；活跃度较大，累计声誉上升明显时，则曲线斜率较大。这样就可以在两次计算大大降低波动性下保留一定的稳定性与响应性。

## 结论与展望

通过我们的声誉累计模型的建立，我们将随机的声誉序列特性与累计声誉结果之间建立了一个长期时间有序的曲线，同时对获得特定声誉值的活跃频率和所需时间之间也建立了良好的指数下降的联系。上面的分析中可以得到以下特性：

- 声誉累计模型具有良好的响应性和分辨度

  声誉模型在p=0.2~0.8的测试数据能够明确地分辨出不同p下行为地差别。同时对于较大和较小的数据，短期有明显的上升与下降，但在长期又能适当消除影响。这对于实际应用非常有意义。

- 声誉累计模型具有广泛的适用性

  经过测试表明声誉累计模型在p=0.2~0.8间均能获得理想的结果，同时可以通过调节回归参数来获得不同的回归衰减，既能适当反映偶然一天的离开，也能在适当时间之后将长期不上线的用户声誉清空。

- 声誉模型具有相当的健壮性

  数据模型在统计数据中方差较为稳定，虽然有少许波动，但保证了整体上升的趋势，表明有较好的可用性。

- 声誉模型具有合适的灵敏性

  数据模型参数中α1和γ对结果影响非常明显，而时间尺度等因素则对结果影响很小，能够满足多种不同场合的需求。

- 声誉模型结果相对稳定，可以用于奖池等激励的分发基础

  声誉模型通过优化之后计算的时间复杂度较好，可以支持进一步奖池分配等行为。同时结果分布较为稳定，在反映用户行为的同时不至于造成严重的两极分化。

- 声誉系统具有历史可回溯性，符合去中心化的需求

在此之上，我们希望对模型进行进一步的迭代，以更加有效地与商家结合：

- 目前的算法是基于单人声誉变化进行的计算。对于不同分布的用户样本，虽然现下不需要进行全样本计算，但在以下两种情况下需要考虑：
  - 涉及用户互动等行为的动态声誉模型模拟
  - 基于全体声誉而进行的如奖励分配等活动

- 通过隐式马尔科夫链建立相应的任务系统并获得完成难度、消耗时间等因子，并进行有任务系统的系统演化，获得更加有效的用户数据
- 在实际区块链上进行验证，确定其实际相关性，对理论模型参数进行深层次优化，增强其可用性。

## Reference

[^1]: https://www.reuters.com/brandfeatures/venture-capital/article?id=63066
[^2]: https://blog.hubspot.com/service/customer-loyalty
[^3]: http://money.163.com/18/0706/09/DM197SM5002580T4.html
[^4]: https://www.douban.com/note/598348163/
[^5]: https://www.huxiu.com/article/110944.html
[^6]: Satoshi Nakamoto,2007,Bitcoin: A Peer-to-Peer Electronic Cash System
[^7]: http://www.dataivy.cn/blog/tag/%E5%AE%8B%E5%A4%A9%E9%BE%99/
[^8]: Morris MD.1991,Factorial sampling plans for preliminary computational experiments,**Technometrics**,33:161~174